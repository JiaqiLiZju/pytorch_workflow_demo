{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([128, 4, 1000])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.rand((128, 4, 1000))\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm1d(out_planes, eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "BasicConv(\n  (conv): Conv1d(4, 64, kernel_size=(7,), stride=(1,), bias=False)\n  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n  (relu): ReLU()\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = BasicConv(4, 64, 7)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([128, 64, 994])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_BasicConv = m(input_tensor)\n",
    "output_BasicConv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flatten()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Flatten()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 63616])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_Flatten = m(output_BasicConv)\n",
    "output_Flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "            )\n",
    "        self.pool_types = pool_types\n",
    "\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = None\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type=='avg':\n",
    "                avg_pool = F.avg_pool1d( x, x.size(2), stride=x.size(2))\n",
    "                channel_att_raw = self.mlp( avg_pool )\n",
    "            elif pool_type=='max':\n",
    "                max_pool = F.max_pool1d( x, x.size(2), stride=x.size(2))\n",
    "                channel_att_raw = self.mlp( max_pool )\n",
    "            # elif pool_type=='lp':\n",
    "            #     lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "            #     channel_att_raw = self.mlp( lp_pool )\n",
    "            # elif pool_type=='lse':\n",
    "            #     # LSE pool only\n",
    "            #     lse_pool = logsumexp_2d(x)\n",
    "            #     channel_att_raw = self.mlp( lse_pool )\n",
    "\n",
    "            if channel_att_sum is None:\n",
    "                channel_att_sum = channel_att_raw\n",
    "            else:\n",
    "                channel_att_sum = channel_att_sum + channel_att_raw\n",
    "\n",
    "        scale = torch.sigmoid( channel_att_sum ).unsqueeze(2).expand_as(x)\n",
    "        return x * scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChannelGate(\n",
       "  (mlp): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Linear(in_features=64, out_features=4, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=4, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = ChannelGate(64)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 64, 994])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ChannelGate = m(output_BasicConv)\n",
    "output_ChannelGate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000, 0.0546, 0.3367, 0.0000, 0.0000, 0.4305, 0.0049,\n",
       "          0.0992],\n",
       "         [0.0000, 0.0000, 0.1492, 0.0000, 0.0000, 0.8467, 0.0000, 0.0000, 0.5111,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.1184, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.3481, 0.1092, 0.0000, 0.0210, 0.1440, 0.0000, 0.2298, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0519, 0.6489, 0.0000, 0.1636, 0.3114, 0.0579, 0.0000,\n",
       "          0.2451]], grad_fn=<SliceBackward>),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0386, 0.2379, 0.0000, 0.0000, 0.3042, 0.0034,\n",
       "          0.0701],\n",
       "         [0.0000, 0.0000, 0.0276, 0.0000, 0.0000, 0.1565, 0.0000, 0.0000, 0.0945,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0588, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.1274, 0.0400, 0.0000, 0.0077, 0.0527, 0.0000, 0.0841, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0290, 0.3619, 0.0000, 0.0912, 0.1737, 0.0323, 0.0000,\n",
       "          0.1367]], grad_fn=<SliceBackward>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_BasicConv[0, :5, :10], output_ChannelGate[0, :5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp_2d(tensor):\n",
    "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
    "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
    "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChannelPool()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = ChannelPool()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2, 994])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ChannelPool = m(output_BasicConv)\n",
    "output_ChannelPool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = torch.sigmoid(x_out) # broadcasting\n",
    "        return x * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpatialGate(\n",
       "  (compress): ChannelPool()\n",
       "  (spatial): BasicConv(\n",
       "    (conv): Conv1d(2, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "    (bn): BatchNorm1d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = SpatialGate()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 64, 994])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_SpatialGate = m(output_BasicConv)\n",
    "output_SpatialGate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000, 0.0546, 0.3367, 0.0000, 0.0000, 0.4305, 0.0049,\n",
       "          0.0992],\n",
       "         [0.0000, 0.0000, 0.1492, 0.0000, 0.0000, 0.8467, 0.0000, 0.0000, 0.5111,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.1184, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.3481, 0.1092, 0.0000, 0.0210, 0.1440, 0.0000, 0.2298, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0519, 0.6489, 0.0000, 0.1636, 0.3114, 0.0579, 0.0000,\n",
       "          0.2451]], grad_fn=<SliceBackward>),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0165, 0.0856, 0.0000, 0.0000, 0.3674, 0.0018,\n",
       "          0.0254],\n",
       "         [0.0000, 0.0000, 0.1324, 0.0000, 0.0000, 0.3282, 0.0000, 0.0000, 0.1894,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0301, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.3399, 0.0969, 0.0000, 0.0053, 0.0558, 0.0000, 0.1961, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0461, 0.1958, 0.0000, 0.0634, 0.2303, 0.0494, 0.0000,\n",
       "          0.0628]], grad_fn=<SliceBackward>))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_BasicConv[0, :5, :10], output_SpatialGate[0, :5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.no_spatial=no_spatial\n",
    "        if not no_spatial:\n",
    "            self.SpatialGate = SpatialGate()\n",
    "    def forward(self, x):\n",
    "        x_out = self.ChannelGate(x)\n",
    "        if not self.no_spatial:\n",
    "            x_out = self.SpatialGate(x_out)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBAM(\n",
       "  (ChannelGate): ChannelGate(\n",
       "    (mlp): Sequential(\n",
       "      (0): Flatten()\n",
       "      (1): Linear(in_features=64, out_features=4, bias=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=4, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (SpatialGate): SpatialGate(\n",
       "    (compress): ChannelPool()\n",
       "    (spatial): BasicConv(\n",
       "      (conv): Conv1d(2, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)\n",
       "      (bn): BatchNorm1d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = CBAM(64)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 64, 994])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_CBAM = m(output_BasicConv)\n",
    "output_CBAM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0000, 0.0546, 0.3367, 0.0000, 0.0000, 0.4305, 0.0049,\n",
       "          0.0992],\n",
       "         [0.0000, 0.0000, 0.1492, 0.0000, 0.0000, 0.8467, 0.0000, 0.0000, 0.5111,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.1184, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.3481, 0.1092, 0.0000, 0.0210, 0.1440, 0.0000, 0.2298, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0519, 0.6489, 0.0000, 0.1636, 0.3114, 0.0579, 0.0000,\n",
       "          0.2451]], grad_fn=<SliceBackward>),\n",
       " tensor([[0.0000, 0.0000, 0.0000, 0.0143, 0.0853, 0.0000, 0.0000, 0.0977, 0.0011,\n",
       "          0.0257],\n",
       "         [0.0000, 0.0000, 0.0277, 0.0000, 0.0000, 0.2059, 0.0000, 0.0000, 0.1083,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0302, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0851, 0.0206, 0.0000, 0.0049, 0.0357, 0.0000, 0.0476, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0124, 0.1961, 0.0000, 0.0513, 0.0948, 0.0152, 0.0000,\n",
       "          0.0734]], grad_fn=<SliceBackward>))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_BasicConv[0, :5, :10], output_CBAM[0, :5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channel, reduction_ratio=16, num_layers=1):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_activation = gate_activation\n",
    "        self.gate_c = nn.Sequential()\n",
    "        self.gate_c.add_module( 'flatten', Flatten() )\n",
    "        gate_channels = [gate_channel]\n",
    "        gate_channels += [gate_channel // reduction_ratio] * num_layers\n",
    "        gate_channels += [gate_channel]\n",
    "        for i in range( len(gate_channels) - 2 ):\n",
    "            self.gate_c.add_module( 'gate_c_fc_%d'%i, nn.Linear(gate_channels[i], gate_channels[i+1]) )\n",
    "            self.gate_c.add_module( 'gate_c_bn_%d'%(i+1), nn.BatchNorm1d(gate_channels[i+1]) )\n",
    "            self.gate_c.add_module( 'gate_c_relu_%d'%(i+1), nn.ReLU() )\n",
    "        self.gate_c.add_module( 'gate_c_fc_final', nn.Linear(gate_channels[-2], gate_channels[-1]) )\n",
    "    def forward(self, in_tensor):\n",
    "        avg_pool = F.avg_pool2d( in_tensor, in_tensor.size(2), stride=in_tensor.size(2) )\n",
    "        return self.gate_c( avg_pool ).unsqueeze(2).unsqueeze(3).expand_as(in_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self, gate_channel, reduction_ratio=16, dilation_conv_num=2, dilation_val=4):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        self.gate_s = nn.Sequential()\n",
    "        self.gate_s.add_module( 'gate_s_conv_reduce0', nn.Conv1d(gate_channel, gate_channel//reduction_ratio, kernel_size=1))\n",
    "        self.gate_s.add_module( 'gate_s_bn_reduce0',\tnn.BatchNorm1d(gate_channel//reduction_ratio) )\n",
    "        self.gate_s.add_module( 'gate_s_relu_reduce0',nn.ReLU() )\n",
    "        for i in range( dilation_conv_num ):\n",
    "            self.gate_s.add_module( 'gate_s_conv_di_%d'%i, nn.Conv1d(gate_channel//reduction_ratio, gate_channel//reduction_ratio, kernel_size=3, padding=dilation_val, dilation=dilation_val) )\n",
    "            self.gate_s.add_module( 'gate_s_bn_di_%d'%i, nn.BatchNorm1d(gate_channel//reduction_ratio) )\n",
    "            self.gate_s.add_module( 'gate_s_relu_di_%d'%i, nn.ReLU() )\n",
    "        self.gate_s.add_module( 'gate_s_conv_final', nn.Conv1d(gate_channel//reduction_ratio, 1, kernel_size=1) )\n",
    "    def forward(self, in_tensor):\n",
    "        return self.gate_s( in_tensor ).expand_as(in_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAM(nn.Module):\n",
    "    def __init__(self, gate_channel):\n",
    "        super(BAM, self).__init__()\n",
    "        self.channel_att = ChannelGate(gate_channel)\n",
    "        self.spatial_att = SpatialGate(gate_channel)\n",
    "    def forward(self,in_tensor):\n",
    "        att = 1 + F.sigmoid( self.channel_att(in_tensor) * self.spatial_att(in_tensor) )\n",
    "        return att * in_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37364bitanaconda3virtualenva73746da2f64482da3ffd974e712932f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}